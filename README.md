# Finetune-Llama3.1-8b_Unsloth


 fine-tuning a LLaMA 3.1 8B model using Unslothâ€™s hybrid quantization (4-bit for model weights + FP16/BF16 for computations), with alpaca-cleaned dataset, achieving a 2x reduction in training time with just a few steps!
