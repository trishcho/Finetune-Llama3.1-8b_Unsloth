# Finetune-Llama3.1-8b_Unsloth


![Screenshot 2024-09-25 at 1 45 28 AM](https://github.com/user-attachments/assets/90a01d3a-d647-455f-bebf-7d9f934b8b09)
![Screenshot 2024-09-25 at 1 45 56 AM](https://github.com/user-attachments/assets/785fe915-6dc6-4ce0-820d-5dd85a844d4e)


 fine-tuning a LLaMA 3.1 8B model using Unsloth’s hybrid quantization (4-bit for model weights + FP16/BF16 for computations), with alpaca-cleaned dataset, achieving a 2x reduction in training time with just a few steps!
